{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7bd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy.matlib as mt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import eigh\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.linalg import eigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blockbuster(Y, N_clust, k = None):\n",
    "\n",
    "    # compute the sample covariance matrix \n",
    "    S = np.cov(Y, rowvar=False)\n",
    "\n",
    "    # compute the largest eigenvalues\n",
    "    eigvals, eigvecs = eigh(S, subset_by_index=[S.shape[0] - N_clust, S.shape[0] - 1])\n",
    "\n",
    "    # apply Eucledian norm\n",
    "    X = eigvecs / np.linalg.norm(eigvecs, axis=1, keepdims=True)\n",
    "\n",
    "    # apply Kmeans algorithm for N = N_clust\n",
    "    kmeans = KMeans(n_clusters=N_clust, random_state=0).fit(X)\n",
    "    \n",
    "    # return the partitions\n",
    "    return kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63616520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_blockbuster(R, bandwidth=\"silverman\", kernel='gaussian', k = None):\n",
    "\n",
    "    # compute the sample covariance matrix\n",
    "    S = np.cov(R, rowvar=False)\n",
    "    \n",
    "    # spectral decomposition of the sample covariance matrix\n",
    "    eigvals, _ = eigh(S)\n",
    "    \n",
    "    # apply eucledian norm of sorted eigenvalues\n",
    "    Ve = np.sort(eigvals)[::-1]\n",
    "    Y = Ve / np.linalg.norm(Ve)\n",
    "    \n",
    "    # apply KDE\n",
    "    kde = gaussian_kde(Y, bw_method=bandwidth)\n",
    "    y_grid = np.linspace(min(Y), max(Y), 1000)\n",
    "    kde_values = kde(y_grid)\n",
    "    \n",
    "    # find number of local minima\n",
    "    local_minima = np.diff(np.sign(np.diff(kde_values))).nonzero()[0] + 1\n",
    "    M = len(local_minima)\n",
    "    \n",
    "    # run blockbuster algorithm with N_clust = K = M + 1\n",
    "    K = M + 1\n",
    "    return blockbuster(R, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneralisedCVC(Y, K_clust=None, N_clust=None, bandwidth='silverman', kernel='gaussian', k=None):\n",
    "\"\"\"\n",
    "Contains code of https://github.com/pald22/covShrinkage:\n",
    "Ledoit, O. and Wolf, M. (2004a). \n",
    "Honey, I shrunk the sample covariance matrix. Journal of Portfolio Management, 30(4):110â€“119.\n",
    "\"\"\"\n",
    "    # de-mean returns if required\n",
    "    N, p = Y.shape        \n",
    "    if k is None or math.isnan(k): \n",
    "        mean = Y.mean(axis=0)\n",
    "        Y = Y.sub(mean, axis=1)                               \n",
    "        k = 1\n",
    "    n = N - k  \n",
    "    \n",
    "    # choose how to determine the clusters\n",
    "    if K_clust is not None:\n",
    "        K_clust = np.array(K_clust)  # predetermined clusters\n",
    "    else:\n",
    "        if N_clust is None:\n",
    "            K_clust = adaptive_blockbuster(Y, bandwidth=bandwidth, kernel=kernel)  # adaptive Blockbuster\n",
    "        else:\n",
    "            K_clust = blockbuster(Y, N_clust)  # Blockbuster\n",
    "    \n",
    "    # compute sample covariance marix\n",
    "    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(), Y.to_numpy())) / n \n",
    "    \n",
    "    # create target matrix\n",
    "    F_GCVC = np.zeros((p, p))\n",
    "    \n",
    "    # cluster information\n",
    "    unique_clusters = np.unique(K_clust)\n",
    "    K = len(unique_clusters)\n",
    "    \n",
    "    # diagonal clusters\n",
    "    for l in unique_clusters:\n",
    "        cluster_l_indices = np.where(K_clust == l)[0]\n",
    "        Nl = len(cluster_l_indices)\n",
    "        \n",
    "        cluster_l_sample = sample.iloc[cluster_l_indices, cluster_l_indices]\n",
    "        meanvar_l = np.mean(np.diag(cluster_l_sample))\n",
    "        meancov_l = (np.sum(cluster_l_sample.to_numpy()) - np.sum(np.diag(cluster_l_sample))) / (Nl * (Nl - 1))\n",
    "        \n",
    "        F_GCVC[np.ix_(cluster_l_indices, cluster_l_indices)] = meanvar_l * np.eye(Nl) + meancov_l * (1 - np.eye(Nl))\n",
    "        \n",
    "        # off-diagonal clusters\n",
    "        for m in unique_clusters:\n",
    "            if l != m:\n",
    "                cluster_m_indices = np.where(K_clust == m)[0]\n",
    "                Nm = len(cluster_m_indices)\n",
    "                \n",
    "                cov_lm = sample.iloc[cluster_l_indices, cluster_m_indices]\n",
    "                nu_lm = np.mean(cov_lm.to_numpy())\n",
    "                F_GCVC[np.ix_(cluster_l_indices, cluster_m_indices)] = nu_lm * np.ones((Nl, Nm))\n",
    "                \n",
    "    # estimate the parameter pi\n",
    "    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n",
    "    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n    \n",
    "    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n",
    "    pihat = sum(piMat.sum())   \n",
    "    \n",
    "    # estimate the parameter gamma \n",
    "    gammahat = np.linalg.norm(sample.to_numpy()-F_GCVC,ord = 'fro')**2\n",
    "    \n",
    "    # estimta rho\n",
    "    rho_diag = (sample2.sum().sum()-np.trace(sample.to_numpy())**2)/p;\n",
    "    \n",
    "    # off-diagonal part of the parameter that we call rho \n",
    "    sum1=Y.sum(axis=1)\n",
    "    sum2=Y2.sum(axis=1)\n",
    "    temp = (np.multiply(sum1.to_numpy(),sum1.to_numpy())-sum2)\n",
    "    rho_off1 = np.sum(np.multiply(temp,temp))/(p*n)\n",
    "    rho_off2 = (sample.sum().sum()-np.trace(sample.to_numpy()))**2/p\n",
    "    rho_off = (rho_off1-rho_off2)/(p-1)\n",
    "    \n",
    "    # compute shrinkage intensity\n",
    "    rhohat = rho_diag + rho_off\n",
    "    \n",
    "    # compute shrinkage intensity\n",
    "    kappahat=(pihat-rhohat)/gammahat\n",
    "    shrinkage=max(0,min(1,kappahat/n))\n",
    "    \n",
    "    # compute shrinkage estimator\n",
    "    sigmahat=shrinkage*F_GCVC+(1-shrinkage)*sample\n",
    "    \n",
    "    # return the estimated covariance matrix, the shrinkage intensity, and the amount of clusters\n",
    "    return sigmahat, shrinkage, K"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
